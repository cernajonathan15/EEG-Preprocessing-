{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import cProfile\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import njit\n",
    "import mne\n",
    "import nibabel\n",
    "from multiprocessing import Pool\n",
    "from hmmlearn import hmm\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Optimal State (EC): 7.24\n",
      "Median Optimal State (EC): 7.00\n",
      "Number of outliers for EC: 0\n",
      "Average Optimal State (EO): 6.73\n",
      "Median Optimal State (EO): 5.00\n",
      "Number of outliers for EO: 0\n"
     ]
    }
   ],
   "source": [
    "# Base directory for output files\n",
    "base_dir = '/home/cerna3/neuroconn/data/out/subjects/'\n",
    "participants = ['101', '102', '103', '104', '105', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120',\n",
    "                '202', '205', '206', '207', '208', '209', '210', '211', '214', '215', '216', '217', '218', '219', '221',\n",
    "                '401', '402', '403', '404', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416']\n",
    "\n",
    "modes = ['EC', 'EO']\n",
    "\n",
    "optimal_states = {mode: [] for mode in modes}  # Store optimal states for each mode\n",
    "\n",
    "# Iterate through participants and modes\n",
    "for participant in participants:\n",
    "    for mode in modes:\n",
    "        file_path = os.path.join(base_dir, participant, mode, f\"aic_bic_{participant}_{mode}.txt\")\n",
    "\n",
    "        # Check if file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Read file and extract optimal state\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"Optimal state (Average):\"):\n",
    "                        optimal_state = int(line.split()[-1])\n",
    "                        optimal_states[mode].append(optimal_state)\n",
    "                        break\n",
    "        else:\n",
    "            print(f\"File missing for participant {participant}, mode {mode}\")\n",
    "\n",
    "# Calculate and print average and median optimal states for each mode\n",
    "for mode, states in optimal_states.items():\n",
    "    mean_optimal_state = np.mean(states)\n",
    "    std_optimal_state = np.std(states)\n",
    "    outliers = [participant for participant, state in zip(participants, states) if abs(state - mean_optimal_state) > 3 * std_optimal_state]\n",
    "    median_optimal_state = sorted(states)[len(states) // 2] if len(states) % 2 != 0 else (sorted(states)[len(states) // 2 - 1] + sorted(states)[len(states) // 2]) / 2\n",
    "    print(f\"Average Optimal State ({mode}): {mean_optimal_state:.2f}\")\n",
    "    print(f\"Median Optimal State ({mode}): {median_optimal_state:.2f}\")\n",
    "    print(f\"Number of outliers for {mode}: {len(outliers)}\")\n",
    "    if outliers:\n",
    "        print(f\"Outlier IDs for {mode}: {', '.join(outliers)}\")\n",
    "\n",
    "    # Save the median optimal state to a file\n",
    "    output_file_path = os.path.join(base_dir, f\"median_optimal_state_{mode}.txt\")\n",
    "    with open(output_file_path, 'w') as out_file:\n",
    "        out_file.write(f\"Median Optimal State ({mode}): {median_optimal_state:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 416 | Mode: EO | Participant Progress: 0.05 min | Avg Time/Participant: 0.03 min\n",
      "All HMM fittings completed.\n",
      "Total processing time: 2.36 minutes. Average time per participant: 0.03 minutes.\n"
     ]
    }
   ],
   "source": [
    "# HMM Fitting \n",
    "\n",
    "# Define the base directory where participant folders are located\n",
    "files_in = '../data/in/subjects/'\n",
    "files_out = '../data/out/subjects/'\n",
    "output_dir_base = '/home/cerna3/neuroconn/data/out/subjects'\n",
    "\n",
    "total_participants = len(participants) * len(modes)\n",
    "current_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "def check_correlation_range(corr_matrix):\n",
    "    \"\"\"Check if the correlation matrix values are within the range [-1, 1].\"\"\"\n",
    "    if np.any(corr_matrix < -1) or np.any(corr_matrix > 1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_data(data, context):\n",
    "    \"\"\"Validate the input data and ensure it does not contain NaNs or empty slices.\"\"\"\n",
    "    if data.size == 0:\n",
    "        raise ValueError(f\"Empty data encountered in {context}.\")\n",
    "    if np.isnan(data).any():\n",
    "        raise ValueError(f\"NaN values encountered in {context}.\")\n",
    "    if np.isinf(data).any():\n",
    "        raise ValueError(f\"Infinite values encountered in {context}.\")\n",
    "    return data\n",
    "\n",
    "# Uncomment if troubleshooting is needed\n",
    "# Initialize counters for empty data cases\n",
    "# total_initial_empty_cases = 0\n",
    "# total_replaced_empty_cases = 0\n",
    "# total_remaining_empty_cases = 0\n",
    "\n",
    "for participant in participants:\n",
    "    participant_start_time = time.time()  # Record start time for the participant\n",
    "    for mode in modes:\n",
    "        dir_in = files_in + participant + '/' + mode + '/'\n",
    "        dir_out = files_out + participant + \"/\" + mode + '/'\n",
    "\n",
    "        try:\n",
    "            # Load orthogonalized data\n",
    "            orthogonalized_data = np.load(dir_out + \"orth.npy\")\n",
    "            orthogonalized_data = validate_data(orthogonalized_data, f\"orthogonalized data for participant {participant}, mode {mode}\")\n",
    "            \n",
    "            features = np.mean(orthogonalized_data, axis=2)\n",
    "            features = validate_data(features, f\"mean features for participant {participant}, mode {mode}\")\n",
    "            features = np.ma.masked_invalid(features).filled(np.mean(features, axis=0))\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(features)\n",
    "\n",
    "            median_optimal_state = np.median(optimal_states[mode])\n",
    "            model = hmm.GaussianHMM(n_components=int(median_optimal_state), n_iter=50, \n",
    "                                    covariance_type='full', tol=1e-7, verbose=False, \n",
    "                                    params='st', init_params='stmc')\n",
    "            model.fit(features)\n",
    "            state_sequence = model.predict(features)\n",
    "            state_probs = model.predict_proba(features)\n",
    "\n",
    "            output_dir = os.path.join(output_dir_base, participant, mode)\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            np.save(os.path.join(output_dir, f\"{participant}_state_sequence.npy\"), state_sequence)\n",
    "            np.save(os.path.join(output_dir, f\"{participant}_state_probs.npy\"), state_probs)\n",
    "\n",
    "            # CALCULATE TEMPORAL FEATURES\n",
    "            \n",
    "            # Compute fractional occupancy: fraction of time spent in each state\n",
    "            fractional_occupancy = np.array([np.sum(state_sequence == i) / len(state_sequence) for i in range(int(median_optimal_state))])\n",
    "\n",
    "            # Compute transition probabilities\n",
    "            transition_counts = np.zeros((int(median_optimal_state), int(median_optimal_state)))\n",
    "            for (i, j) in zip(state_sequence[:-1], state_sequence[1:]):\n",
    "                transition_counts[i, j] += 1\n",
    "            transition_probabilities = transition_counts / np.sum(transition_counts, axis=1, keepdims=True)\n",
    "\n",
    "            # Compute mean lifetime (dwell time) in each state: average time spent in each state before transitioning\n",
    "            mean_lifetime = np.zeros(int(median_optimal_state))\n",
    "            for i in range(int(median_optimal_state)):\n",
    "                # Identify the indices where state changes\n",
    "                change_indices = np.where(np.diff(state_sequence == i, prepend=False, append=False))[0]\n",
    "                # Calculate segment lengths by differencing indices of changes; add 1 because diff loses 1\n",
    "                segment_lengths = np.diff(change_indices) + 1\n",
    "                # Compute mean segment length for state i\n",
    "                mean_lifetime[i] = np.mean(segment_lengths) if len(segment_lengths) > 0 else 0\n",
    "\n",
    "            # Mean Interval Length: average time between consecutive occurrences of each state\n",
    "            mean_interval_length = np.zeros(int(median_optimal_state))\n",
    "            for k in range(int(median_optimal_state)):\n",
    "                # Boolean array where True is the state 'k'\n",
    "                is_state_k = state_sequence == k\n",
    "                # Time points where state is 'k'\n",
    "                time_points_k = np.where(is_state_k)[0]\n",
    "                # Compute time differences between consecutive occurrences of state 'k'\n",
    "                intervals_k = np.diff(time_points_k) - 1\n",
    "                # Compute the mean of these intervals, accounting for the case where state 'k' does not repeat\n",
    "                mean_interval_length[k] = np.mean(intervals_k) if len(intervals_k) > 0 else 0\n",
    "                \n",
    "            np.savez(os.path.join(output_dir, f\"{participant}_temporal_features.npz\"), \n",
    "                     fractional_occupancy=fractional_occupancy, \n",
    "                     transition_probabilities=transition_probabilities,\n",
    "                     mean_lifetime=mean_lifetime, \n",
    "                     mean_interval_length=mean_interval_length)\n",
    "                    \n",
    "            # CALCULATE SPATIAL FEATURES (FUNCTIONAL CONNECTIVITY)\n",
    "            \n",
    "            def calculate_functional_connectivity(orthogonalized_data, state_sequence, median_optimal_state):\n",
    "                correlation_matrices = {}\n",
    "                positive_correlations = {}\n",
    "                negative_correlations = {}\n",
    "\n",
    "                initial_empty_case_count = 0  # Counter for initial empty data cases\n",
    "                replaced_empty_case_count = 0  # Counter for replaced empty data cases\n",
    "\n",
    "                for state in range(int(median_optimal_state)):\n",
    "                    state_indices = np.where(state_sequence == state)[0]\n",
    "\n",
    "                    block_starts = np.where(np.diff(state_indices) > 1)[0] + 1\n",
    "                    block_starts = np.insert(block_starts, 0, 0)\n",
    "                    block_ends = np.append(block_starts[1:] - 1, len(state_indices) - 1)\n",
    "                    state_blocks = zip(state_indices[block_starts], state_indices[block_ends] + 1)\n",
    "\n",
    "                    for i, (start_index, end_index) in enumerate(state_blocks):\n",
    "                        state_data = orthogonalized_data[:, :, start_index:end_index]\n",
    "\n",
    "                        if state_data.size == 0:\n",
    "                            initial_empty_case_count += 1  # Increment initial empty case count\n",
    "                            # Replace empty data with the mean of the input data\n",
    "                            state_data = np.mean(orthogonalized_data, axis=2, keepdims=True) \n",
    "                            replaced_empty_case_count += 1\n",
    "\n",
    "                        state_data = validate_data(state_data, f\"state data for state {state}, block {i}, participant {participant}, mode {mode}\")\n",
    "\n",
    "                        # Select the time and sample dimensions for correlation\n",
    "                        reshaped_data = state_data.swapaxes(1, 2).reshape(102, -1)\n",
    "\n",
    "                        # Impute NaNs if necessary\n",
    "                        if np.isnan(state_data).any():\n",
    "                            reshaped_data = np.nan_to_num(reshaped_data)\n",
    "\n",
    "                        corr_matrix = np.corrcoef(reshaped_data)\n",
    "                        if not check_correlation_range(corr_matrix):\n",
    "                            raise ValueError(f\"Correlation values out of range in state {state}, block {i} for participant {participant}, mode {mode}. Check data preprocessing.\")\n",
    "\n",
    "                        correlation_matrices[(state, i)] = corr_matrix\n",
    "\n",
    "                        upper_tri_indices = np.triu_indices_from(corr_matrix, k=1)\n",
    "                        positive_correlations[(state, i)] = corr_matrix[upper_tri_indices][corr_matrix[upper_tri_indices] > 0]\n",
    "                        negative_correlations[(state, i)] = corr_matrix[upper_tri_indices][corr_matrix[upper_tri_indices] < 0]\n",
    "\n",
    "                remaining_empty_case_count = sum(1 for key, value in correlation_matrices.items() if value.size == 0)\n",
    "\n",
    "                return correlation_matrices, positive_correlations, negative_correlations, initial_empty_case_count, replaced_empty_case_count, remaining_empty_case_count\n",
    "\n",
    "            # CALCULATE SPATIAL FEATURES (FUNCTIONAL CONNECTIVITY)\n",
    "            correlation_matrices, positive_correlations, negative_correlations, initial_empty_case_count, replaced_empty_case_count, remaining_empty_case_count = calculate_functional_connectivity(\n",
    "                orthogonalized_data, state_sequence, median_optimal_state)\n",
    "            \n",
    "            # Uncomment if troubleshooting is needed\n",
    "            # total_initial_empty_cases += initial_empty_case_count\n",
    "            # total_replaced_empty_cases += replaced_empty_case_count\n",
    "            # total_remaining_empty_cases += remaining_empty_case_count\n",
    "\n",
    "            correlation_matrices_file = os.path.join(output_dir, f\"{participant}_correlation_matrices.npy\")\n",
    "            positive_correlations_file = os.path.join(output_dir, f\"{participant}_positive_correlations.npy\")\n",
    "            negative_correlations_file = os.path.join(output_dir, f\"{participant}_negative_correlations.npy\")\n",
    "            # Convert tuple keys to strings\n",
    "            arrays_to_save = {}\n",
    "            for key, value in correlation_matrices.items():\n",
    "                key_str = \"_\".join(map(str, key))\n",
    "                arrays_to_save[key_str] = value\n",
    "            np.savez(correlation_matrices_file, **arrays_to_save)\n",
    "            np.savez(positive_correlations_file, positive_correlations)\n",
    "            np.savez(negative_correlations_file, negative_correlations)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing participant {participant}, mode {mode}: {e}\")\n",
    "        \n",
    "        current_count += 1\n",
    "        participant_elapsed_time = (time.time() - participant_start_time) / 60\n",
    "        total_elapsed_time = (time.time() - start_time) / 60\n",
    "        avg_time_per_participant = total_elapsed_time / current_count\n",
    "        #progress_percent = (current_count / total_participants) * 100  # Uncomment if the need to keep track of participant's processing progress arises\n",
    "        sys.stdout.write(f\"\\rProcessing {participant} | Mode: {mode} | \"\n",
    "                         f\"Participant Progress: {participant_elapsed_time:.2f} min | \"\n",
    "                         #f\"Overall Progress: {progress_percent:.2f}% | \" # Uncomment if the need to keep track of participant's processing progress arises\n",
    "                         f\"Avg Time/Participant: {avg_time_per_participant:.2f} min\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print(\"\\nAll HMM fittings completed.\")\n",
    "total_time_taken = (time.time() - start_time) / 60\n",
    "print(f\"Total processing time: {total_time_taken:.2f} minutes. Average time per participant: {avg_time_per_participant:.2f} minutes.\")\n",
    "\n",
    "# Uncomment if troubleshooting is needed\n",
    "# print(f\"Total initial empty data cases: {total_initial_empty_cases}\")\n",
    "# print(f\"Total replaced empty data cases: {total_replaced_empty_cases}\")\n",
    "# print(f\"Total remaining empty data cases: {total_remaining_empty_cases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 45/45 participants (100.00%)\n",
      "All participants processed.\n",
      "Total matrices loaded: 11513\n",
      "Number of out-of-bounds matrices during loading: 0.00%\n",
      "Converting correlation matrices to NetworkX graphs...\n",
      "Converting matrix 11513/11513 to graph (100.00%)\n",
      "Graph conversion completed. Total graphs converted: 11513\n",
      "Starting aggregation of edge weights from all windowed graphs...\n",
      "Aggregating edge weights progress: 11513/11513 (100.00%)\n",
      "Total number of edge weights aggregated: 60477789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregation of edge weights completed.\n",
      "Starting bootstrapping on aggregated edge weights...\n"
     ]
    }
   ],
   "source": [
    "# Function to load all correlation matrices with checks for out-of-bounds values\n",
    "def load_all_correlation_matrices(base_dir, participants):\n",
    "    all_matrices = []\n",
    "    total_participants = len(participants)\n",
    "    out_of_bounds_count = 0  # Counter for out-of-bounds matrices\n",
    "    printed_out_of_bounds = False  # Flag to check if an out-of-bounds matrix has been printed\n",
    "\n",
    "    for idx, participant in enumerate(participants):\n",
    "        participant_dir = os.path.join(base_dir, participant)\n",
    "        for mode in [\"EC\", \"EO\"]:\n",
    "            mode_dir = os.path.join(participant_dir, mode)\n",
    "            if os.path.isdir(mode_dir):\n",
    "                filename_pattern = f\"{participant}_correlation_matrices.npy.npz\"\n",
    "                filepath = os.path.join(mode_dir, filename_pattern)\n",
    "                if os.path.exists(filepath):\n",
    "                    data = np.load(filepath, allow_pickle=True)\n",
    "                    for key in data.files:\n",
    "                        matrix = data[key]\n",
    "                        if matrix.ndim == 2:\n",
    "                            # Check for out-of-bounds values and print detailed statistics if found\n",
    "                            if np.any(matrix < -1) or np.any(matrix > 1):\n",
    "                                out_of_bounds_count += 1\n",
    "                                if not printed_out_of_bounds:\n",
    "                                    print(f\"Out-of-bounds values found in file {filepath}, key {key}.\")\n",
    "                                    out_of_bounds_indices = np.where((matrix < -1) | (matrix > 1))\n",
    "                                    print(f\"Out-of-bounds values in matrix: {matrix[out_of_bounds_indices]}\")\n",
    "                                    print(f\"Matrix {participant}, {mode}, {key}: min {np.min(matrix)}, max {np.max(matrix)}, mean {np.mean(matrix)}, std {np.std(matrix)}\")\n",
    "                                    printed_out_of_bounds = True\n",
    "                            all_matrices.append(matrix)\n",
    "                        else:\n",
    "                            print(f\"Non-2D matrix found in file {filepath}, key {key}.\")\n",
    "                else:\n",
    "                    print(f\"File not found: {filepath}\")\n",
    "        print(f\"Processed {idx + 1}/{total_participants} participants ({(idx + 1) / total_participants * 100:.2f}%)\", end='\\r')\n",
    "    \n",
    "    print(\"\\nAll participants processed.\")\n",
    "    print(f\"Total matrices loaded: {len(all_matrices)}\")\n",
    "    print(f\"Number of out-of-bounds matrices during loading: {out_of_bounds_count / len(all_matrices) * 100:.2f}%\")\n",
    "    return all_matrices\n",
    "\n",
    "def determine_optimal_alpha(all_matrices):\n",
    "    print(\"Converting correlation matrices to NetworkX graphs...\")\n",
    "    windowed_graphs = []\n",
    "    total_matrices = len(all_matrices)\n",
    "    for i, matrix in enumerate(all_matrices):\n",
    "        try:\n",
    "            windowed_graphs.append(nx.from_numpy_array(matrix))\n",
    "            # Update progress every 10%\n",
    "            print(f\"Converting matrix {i + 1}/{total_matrices} to graph ({(i + 1) / total_matrices * 100:.2f}%)\", end='\\r')\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping matrix {i + 1}/{total_matrices} due to error: {e}\")\n",
    "    print(f\"\\nGraph conversion completed. Total graphs converted: {len(windowed_graphs)}\")\n",
    "    return aggregated_bootstrapping_and_alpha_threshold(windowed_graphs)\n",
    "\n",
    "@njit\n",
    "def test_alpha_numba(threshold_array, alpha):\n",
    "    # Find indices where the condition is true\n",
    "    row_indices, col_indices = np.where(threshold_array >= alpha) \n",
    "\n",
    "    # Extract values using the indices\n",
    "    valid_connections = np.extract(threshold_array >= alpha, threshold_array)  \n",
    "\n",
    "    count = valid_connections.size\n",
    "    return np.sum(valid_connections) / count if count > 0 else 0\n",
    "\n",
    "def test_alpha_numba_wrapper(threshold_array, alpha): \n",
    "    result = test_alpha_numba(threshold_array, alpha)\n",
    "    return result\n",
    "\n",
    "def aggregated_bootstrapping_and_alpha_threshold(windowed_graphs, num_iterations=10000, num_alphas=100):\n",
    "    if not windowed_graphs:\n",
    "        raise ValueError(\"No valid graphs found for processing.\")\n",
    "    \n",
    "    print(\"Starting aggregation of edge weights from all windowed graphs...\")\n",
    "    all_edge_weights = []\n",
    "    total_graphs = len(windowed_graphs)\n",
    "    edge_count = 0\n",
    "\n",
    "    for i, G in enumerate(windowed_graphs):\n",
    "        graph_edge_weights = [data['weight'] for _, _, data in G.edges(data=True)]\n",
    "        all_edge_weights.extend(graph_edge_weights)\n",
    "        edge_count += len(graph_edge_weights)\n",
    "        print(f\"Aggregating edge weights progress: {i + 1}/{total_graphs} ({(i + 1) / total_graphs * 100:.2f}%)\", end='\\r')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    all_edge_weights = np.array(all_edge_weights)\n",
    "    print(f\"\\nTotal number of edge weights aggregated: {edge_count}\")\n",
    "    print(\"Aggregation of edge weights completed.\")\n",
    "\n",
    "    print(\"Starting bootstrapping on aggregated edge weights...\")\n",
    "    bootstrap_weights = np.zeros_like(all_edge_weights)\n",
    "    for i in range(num_iterations):\n",
    "        random_indices = np.random.randint(0, len(all_edge_weights), size=len(all_edge_weights))\n",
    "        bootstrap_sample = all_edge_weights[random_indices]\n",
    "        bootstrap_weights += bootstrap_sample\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Bootstrapping progress: {i + 1}/{num_iterations} ({(i + 1) / num_iterations * 100:.2f}%)\", end='\\r')\n",
    "    bootstrap_median = np.median(bootstrap_weights / num_iterations)\n",
    "    print(\"\\nBootstrapping completed.\")\n",
    "\n",
    "    if len(all_edge_weights) == 0:\n",
    "        raise ValueError(\"No edge weights found for bootstrapping.\")\n",
    "    \n",
    "    alpha_start = np.percentile(all_edge_weights, 5) / bootstrap_median\n",
    "    alpha_end = np.percentile(all_edge_weights, 95) / bootstrap_median\n",
    "    \n",
    "    # Pre-calculate threshold arrays here (after bootstrap_median is calculated)\n",
    "    print(\"Pre-calculating threshold arrays...\")\n",
    "    threshold_arrays = []\n",
    "    for G in windowed_graphs:\n",
    "        connectivity_array = np.asarray(nx.to_numpy_array(G))\n",
    "        threshold_arrays.append((connectivity_array / bootstrap_median) ** 2)\n",
    "    \n",
    "    print(\"Starting golden-section search for optimal alpha...\")\n",
    "    gr = (np.sqrt(5) + 1) / 2\n",
    "    c = alpha_end - (alpha_end - alpha_start) / gr\n",
    "    d = alpha_start + (alpha_end - alpha_start) / gr\n",
    "\n",
    "    iteration_count = 0\n",
    "    alpha_values = [c, d]\n",
    "    fc_values = []\n",
    "    fd_values = []\n",
    "    while True:\n",
    "        iteration_count += 1\n",
    "        # Calculation using pre-calculated arrays\n",
    "        fc = np.mean([test_alpha_numba_wrapper(arr, c) for arr in threshold_arrays])\n",
    "        fd = np.mean([test_alpha_numba_wrapper(arr, d) for arr in threshold_arrays])\n",
    "        alpha_values.append((c + d) / 2)\n",
    "        fc_values.append(fc)\n",
    "        fd_values.append(fd)\n",
    "\n",
    "        if iteration_count > 1:\n",
    "            relative_threshold = 0.01 * np.std(alpha_values)\n",
    "            if abs(c - d) < relative_threshold:\n",
    "                if len(fc_values) > 5:\n",
    "                    t_stat, p_value = stats.ttest_rel(fc_values[-5:], fd_values[-5:])\n",
    "                    if p_value > 0.05:\n",
    "                        break\n",
    "\n",
    "        print(f\"Testing alphas: {c:.4f}, {d:.4f} - Iteration {iteration_count}\", end='\\r')\n",
    "        if fc < fd:\n",
    "            alpha_end = d\n",
    "            d = c\n",
    "            c = alpha_end - (alpha_end - alpha_start) / gr\n",
    "        else:\n",
    "            alpha_start = c\n",
    "            c = d\n",
    "            d = alpha_start + (alpha_end - alpha_start) / gr\n",
    "\n",
    "    optimal_alpha = np.median(alpha_values)\n",
    "    print(f\"\\nOptimal alpha determined: {optimal_alpha:.4f}\")\n",
    "    print(\"\\nTesting completed\")\n",
    "    return optimal_alpha, bootstrap_median\n",
    "\n",
    "def save_alpha_and_median(base_dir, optimal_alpha, bootstrap_median):\n",
    "    save_path = os.path.join(base_dir, \"alpha_and_median.json\")\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    with open(save_path, 'w') as file:\n",
    "        json.dump({'optimal_alpha': optimal_alpha, 'bootstrap_median': bootstrap_median}, file)\n",
    "    print(f\"Optimal alpha and bootstrap median saved to: {save_path}\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Usage\n",
    "base_dir = \"/home/cerna3/neuroconn/data/out/subjects\"  # Path to your data\n",
    "max_matrices_to_load = 50  # Adjust this value as needed\n",
    "all_matrices = load_all_correlation_matrices(base_dir, participants)\n",
    "optimal_alpha, bootstrap_median = determine_optimal_alpha(all_matrices)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in minutes\n",
    "elapsed_time_minutes = (end_time - start_time) / 60\n",
    "    \n",
    "print(\"Optimal Alpha:\", optimal_alpha)\n",
    "print(\"Bootstrap Median:\", bootstrap_median)\n",
    "print(f\"Total time taken: {elapsed_time_minutes:.2f} minutes\")\n",
    "\n",
    "# Save the optimal alpha and bootstrap median\n",
    "save_alpha_and_median(base_dir, optimal_alpha, bootstrap_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants processed/thresholded: 45/45\n",
      "All participants processed.\n",
      "Processing participant 416, Mode EO: 45/45\n",
      "All participants processed.\n",
      "Participants with all modes processed and thresholded: 101, 102, 103, 104, 105, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 202, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 221, 401, 402, 403, 404, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416\n",
      "Average unthresholded edge count: 658927.37\n",
      "Average thresholded edge count: 646065.43\n",
      "Average number of pruned edges: 12861.93\n",
      "Total time taken: 2.14 minutes\n"
     ]
    }
   ],
   "source": [
    "# Thresholding phase: Apply the standardized optimal alpha value calculated from the aggregated and bootstrapped edges (of all participants) to threshold the corr_matrices for each participant\n",
    "\n",
    "# Load optimal alpha and bootstrap median from JSON file\n",
    "def load_alpha_and_median(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"optimal_alpha\"], data[\"bootstrap_median\"]\n",
    "\n",
    "# Updated file path for the .json\n",
    "json_path = \"/home/cerna3/neuroconn/data/out/subjects/alpha_and_median.json\"\n",
    "optimal_alpha, bootstrap_median = load_alpha_and_median(json_path)\n",
    "\n",
    "def process_participant(participant_dir, optimal_alpha, bootstrap_median):\n",
    "    \"\"\"Processes each participant's data, applies thresholding, and saves results.\"\"\"\n",
    "    participant_id = os.path.split(participant_dir)[-1]\n",
    "    results = {}\n",
    "    success = True\n",
    "    thresholded_correlation_matrices = {}  # Initialize the dictionary to store thresholded matrices\n",
    "\n",
    "    for mode in [\"EC\", \"EO\"]:\n",
    "        mode_dir = os.path.join(participant_dir, mode)\n",
    "        correlation_matrices = {}\n",
    "        results[mode] = {}\n",
    "\n",
    "        for filename in os.listdir(mode_dir):\n",
    "            if filename.startswith(f\"{participant_id}_correlation_matrices\"):\n",
    "                filepath = os.path.join(mode_dir, filename)\n",
    "                data = np.load(filepath)\n",
    "                for key in data.files:\n",
    "                    try:\n",
    "                        tuple_key = tuple(map(int, key.split('_')))\n",
    "                        matrix = data[key]  # Directly use the matrix without Fisher's r-to-Z transformation\n",
    "                        correlation_matrices[tuple_key] = matrix\n",
    "                    except ValueError:\n",
    "                        continue  # Skip keys that don't convert to integers\n",
    "\n",
    "                try:\n",
    "                    thresholded_matrices, pos_corrs, neg_corrs = threshold_functional_connectivity(\n",
    "                        correlation_matrices, optimal_alpha, bootstrap_median\n",
    "                    )\n",
    "                    results[mode]['correlation_matrices'] = correlation_matrices\n",
    "                    results[mode]['thresholded_matrices'] = thresholded_matrices\n",
    "                    thresholded_correlation_matrices.update(thresholded_matrices)  # Store thresholded matrices\n",
    "                    save_thresholded_data(mode_dir, participant_id, thresholded_matrices, pos_corrs, neg_corrs)\n",
    "                except Exception as e:\n",
    "                    success = False\n",
    "                    break\n",
    "\n",
    "    return results, success, thresholded_correlation_matrices  # Return the thresholded matrices along with other results\n",
    "\n",
    "def save_thresholded_data(mode_dir, participant_id, thresholded_matrices, pos_corrs, neg_corrs):\n",
    "    \"\"\"Saves thresholded data in .npz format with '_raw' suffix.\"\"\"\n",
    "    # Initialize dictionaries to store data with string keys\n",
    "    thresholded_arrays_to_save = {}\n",
    "    pos_corrs_to_save = {}\n",
    "    neg_corrs_to_save = {}\n",
    "\n",
    "    # Convert keys from tuples to strings and save corresponding data\n",
    "    for key, matrix in thresholded_matrices.items():\n",
    "        key_str = \"_\".join(map(str, key))  # Converts key tuple to a string\n",
    "        thresholded_arrays_to_save[key_str] = matrix\n",
    "        pos_corrs_to_save[key_str] = pos_corrs[key]\n",
    "        neg_corrs_to_save[key_str] = neg_corrs[key]\n",
    "\n",
    "    # Save the dictionaries using np.savez which can handle multiple arrays in a single file\n",
    "    np.savez(os.path.join(mode_dir, f\"{participant_id}_thresholded_matrices_raw.npz\"), **thresholded_arrays_to_save)\n",
    "    np.savez(os.path.join(mode_dir, f\"{participant_id}_thresholded_pos_corrs_raw.npz\"), **pos_corrs_to_save)\n",
    "    np.savez(os.path.join(mode_dir, f\"{participant_id}_thresholded_neg_corrs_raw.npz\"), **neg_corrs_to_save)\n",
    "\n",
    "@njit\n",
    "def apply_threshold(corr_matrix, optimal_alpha_squared, bootstrap_median):\n",
    "    \"\"\"Applies the threshold to a single correlation matrix using numba for acceleration.\"\"\"\n",
    "    size = corr_matrix.shape[0]\n",
    "    thresholded_matrix = np.zeros_like(corr_matrix)\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            normalized_weight = corr_matrix[i, j] / bootstrap_median\n",
    "            if normalized_weight ** 2 >= optimal_alpha_squared:\n",
    "                thresholded_matrix[i, j] = corr_matrix[i, j]\n",
    "    \n",
    "    return thresholded_matrix\n",
    "\n",
    "def threshold_functional_connectivity(correlation_matrices, optimal_alpha, bootstrap_median):\n",
    "    \"\"\"Applies threshold based on alpha and bootstrap_median to filter correlation matrices.\"\"\"\n",
    "    thresholded_correlation_matrices = {}\n",
    "    thresholded_positive_correlations = {}\n",
    "    thresholded_negative_correlations = {}\n",
    "    optimal_alpha_squared = optimal_alpha ** 2  # Calculate squared alpha for the threshold condition\n",
    "\n",
    "    for key, corr_matrix in correlation_matrices.items():\n",
    "        thresholded_matrix = apply_threshold(corr_matrix, optimal_alpha_squared, bootstrap_median)\n",
    "        thresholded_correlation_matrices[key] = thresholded_matrix\n",
    "\n",
    "        pos_corrs = thresholded_matrix[thresholded_matrix > 0]\n",
    "        neg_corrs = thresholded_matrix[thresholded_matrix < 0]\n",
    "        thresholded_positive_correlations[key] = pos_corrs\n",
    "        thresholded_negative_correlations[key] = neg_corrs\n",
    "\n",
    "    return thresholded_correlation_matrices, thresholded_positive_correlations, thresholded_negative_correlations\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Collecting thresholded matrices for all participants\n",
    "all_thresholded_matrices = {}\n",
    "\n",
    "# Process participants and store results\n",
    "participants_data = {}\n",
    "total_participants = len(participants)\n",
    "processed_participants = [] \n",
    "thresholded_participants = []\n",
    "\n",
    "for idx, participant in enumerate(sorted(participants, key=int)):\n",
    "    participant_dir = os.path.join(base_dir, participant)\n",
    "    results, success, thresholded_matrices = process_participant(participant_dir, optimal_alpha, bootstrap_median)\n",
    "    participants_data[participant] = results\n",
    "\n",
    "    if success:\n",
    "        # Store thresholded matrices in a dictionary, if needed\n",
    "        all_thresholded_matrices[participant] = thresholded_matrices\n",
    "\n",
    "    sys.stdout.write(f\"\\rParticipants processed/thresholded: {idx + 1}/{len(participants)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\nAll participants processed.\")\n",
    "\n",
    "# EDGE COUNT SUMMARY\n",
    "\n",
    "@njit\n",
    "def count_edges(matrix):\n",
    "    \"\"\"Counts the non-zero entries in the matrix that represent edges.\"\"\"\n",
    "    count = 0\n",
    "    size = matrix.shape[0]\n",
    "    for i in range(size):\n",
    "        for j in range(i + 1, size):  # Only count each edge once in an undirected graph\n",
    "            if matrix[i, j] != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# Initialize counters\n",
    "total_unthresholded_edges = 0\n",
    "total_thresholded_edges = 0\n",
    "total_edge_counts = 0\n",
    "thresholded_participants = []\n",
    "no_modes_processed = []\n",
    "one_mode_processed = []\n",
    "\n",
    "for idx, participant in enumerate(sorted(participants_data.keys(), key=int)):\n",
    "    data = participants_data[participant]\n",
    "    processed_modes = [mode for mode, results in data.items() if 'correlation_matrices' in results and 'thresholded_matrices' in results]\n",
    "\n",
    "    if not processed_modes:\n",
    "        no_modes_processed.append(participant)\n",
    "    elif len(processed_modes) == 1:\n",
    "        one_mode_processed.append((participant, processed_modes[0]))\n",
    "    else:\n",
    "        thresholded_participants.append(participant)\n",
    "        # Loop through each processed mode\n",
    "        for mode in processed_modes:\n",
    "            results = data[mode]\n",
    "            unthresholded_graph_edges = 0\n",
    "            thresholded_graph_edges = 0\n",
    "            for key, corr_matrix in results['correlation_matrices'].items():\n",
    "                unthresholded_graph_edges += count_edges(corr_matrix)\n",
    "                thresholded_graph_edges += count_edges(results['thresholded_matrices'][key])\n",
    "\n",
    "            # Update total counts for the participant in each mode\n",
    "            total_unthresholded_edges += unthresholded_graph_edges\n",
    "            total_thresholded_edges += thresholded_graph_edges\n",
    "            total_edge_counts += 1  # This counts the number of mode entries processed, not participants\n",
    "\n",
    "            sys.stdout.write(f\"\\rProcessing participant {participant}, Mode {mode}: {idx + 1}/{len(participants_data)}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "print(\"\\nAll participants processed.\")\n",
    "if no_modes_processed:\n",
    "    print(\"Participants with no modes processed:\", \", \".join(no_modes_processed))\n",
    "if one_mode_processed:\n",
    "    print(\"Participants with only one mode processed:\")\n",
    "    for participant, mode in one_mode_processed:\n",
    "        print(f\"  - {participant} (Mode: {mode})\")\n",
    "if thresholded_participants:\n",
    "    print(\"Participants with all modes processed and thresholded:\", \", \".join(thresholded_participants))\n",
    "\n",
    "# Calculate and print averages if applicable\n",
    "if total_edge_counts > 0:\n",
    "    average_unthresholded = total_unthresholded_edges / total_edge_counts\n",
    "    average_thresholded = total_thresholded_edges / total_edge_counts\n",
    "    average_pruned = average_unthresholded - average_thresholded\n",
    "\n",
    "    print(f\"Average unthresholded edge count: {average_unthresholded:.2f}\")\n",
    "    print(f\"Average thresholded edge count: {average_thresholded:.2f}\")\n",
    "    print(f\"Average number of pruned edges: {average_pruned:.2f}\")\n",
    "else:\n",
    "    print(\"No edge count data available.\")\n",
    "    \n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in minutes\n",
    "elapsed_time_minutes = (end_time - start_time) / 60  # Convert seconds to minutes\n",
    "print(f\"Total time taken: {elapsed_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting within/between network connectivity calculations...\n",
      "Within/Between network connectivity calculated for participant: 416 for modes EC and EO (45/45)\n",
      "Completed calculation of network connectivity for all participants.\n",
      "Total time taken for calculation: 3.54 minutes\n",
      "Starting aggregation of network connectivity...\n",
      "Aggregated network connectivity calculated for participant: 416 for modes EC and EO (45/45).\n",
      "Completed aggregation of network connectivity for all participants.\n",
      "Total time taken for aggregation: 0.46 minutes\n",
      "Successfully saved connectivity results for participant: 101, mode: EC\n",
      "Successfully saved connectivity results for participant: 101, mode: EO\n",
      "Successfully saved connectivity results for participant: 102, mode: EC\n",
      "Successfully saved connectivity results for participant: 102, mode: EO\n",
      "Successfully saved connectivity results for participant: 103, mode: EC\n",
      "Successfully saved connectivity results for participant: 103, mode: EO\n",
      "Successfully saved connectivity results for participant: 104, mode: EC\n",
      "Successfully saved connectivity results for participant: 104, mode: EO\n",
      "Successfully saved connectivity results for participant: 105, mode: EC\n",
      "Successfully saved connectivity results for participant: 105, mode: EO\n",
      "Successfully saved connectivity results for participant: 111, mode: EC\n",
      "Successfully saved connectivity results for participant: 111, mode: EO\n",
      "Successfully saved connectivity results for participant: 112, mode: EC\n",
      "Successfully saved connectivity results for participant: 112, mode: EO\n",
      "Successfully saved connectivity results for participant: 113, mode: EC\n",
      "Successfully saved connectivity results for participant: 113, mode: EO\n",
      "Successfully saved connectivity results for participant: 114, mode: EC\n",
      "Successfully saved connectivity results for participant: 114, mode: EO\n",
      "Successfully saved connectivity results for participant: 115, mode: EC\n",
      "Successfully saved connectivity results for participant: 115, mode: EO\n",
      "Successfully saved connectivity results for participant: 116, mode: EC\n",
      "Successfully saved connectivity results for participant: 116, mode: EO\n",
      "Successfully saved connectivity results for participant: 117, mode: EC\n",
      "Successfully saved connectivity results for participant: 117, mode: EO\n",
      "Successfully saved connectivity results for participant: 118, mode: EC\n",
      "Successfully saved connectivity results for participant: 118, mode: EO\n",
      "Successfully saved connectivity results for participant: 119, mode: EC\n",
      "Successfully saved connectivity results for participant: 119, mode: EO\n",
      "Successfully saved connectivity results for participant: 120, mode: EC\n",
      "Successfully saved connectivity results for participant: 120, mode: EO\n",
      "Successfully saved connectivity results for participant: 202, mode: EC\n",
      "Successfully saved connectivity results for participant: 202, mode: EO\n",
      "Successfully saved connectivity results for participant: 205, mode: EC\n",
      "Successfully saved connectivity results for participant: 205, mode: EO\n",
      "Successfully saved connectivity results for participant: 206, mode: EC\n",
      "Successfully saved connectivity results for participant: 206, mode: EO\n",
      "Successfully saved connectivity results for participant: 207, mode: EC\n",
      "Successfully saved connectivity results for participant: 207, mode: EO\n",
      "Successfully saved connectivity results for participant: 208, mode: EC\n",
      "Successfully saved connectivity results for participant: 208, mode: EO\n",
      "Successfully saved connectivity results for participant: 209, mode: EC\n",
      "Successfully saved connectivity results for participant: 209, mode: EO\n",
      "Successfully saved connectivity results for participant: 210, mode: EC\n",
      "Successfully saved connectivity results for participant: 211, mode: EC\n",
      "Successfully saved connectivity results for participant: 211, mode: EO\n",
      "Successfully saved connectivity results for participant: 214, mode: EC\n",
      "Successfully saved connectivity results for participant: 214, mode: EO\n",
      "Successfully saved connectivity results for participant: 215, mode: EC\n",
      "Successfully saved connectivity results for participant: 215, mode: EO\n",
      "Successfully saved connectivity results for participant: 216, mode: EC\n",
      "Successfully saved connectivity results for participant: 216, mode: EO\n",
      "Successfully saved connectivity results for participant: 217, mode: EC\n",
      "Successfully saved connectivity results for participant: 217, mode: EO\n",
      "Successfully saved connectivity results for participant: 218, mode: EC\n",
      "Successfully saved connectivity results for participant: 218, mode: EO\n",
      "Successfully saved connectivity results for participant: 219, mode: EC\n",
      "Successfully saved connectivity results for participant: 219, mode: EO\n",
      "Successfully saved connectivity results for participant: 221, mode: EC\n",
      "Successfully saved connectivity results for participant: 221, mode: EO\n",
      "Successfully saved connectivity results for participant: 401, mode: EC\n",
      "Successfully saved connectivity results for participant: 401, mode: EO\n",
      "Successfully saved connectivity results for participant: 402, mode: EC\n",
      "Successfully saved connectivity results for participant: 402, mode: EO\n",
      "Successfully saved connectivity results for participant: 403, mode: EC\n",
      "Successfully saved connectivity results for participant: 403, mode: EO\n",
      "Successfully saved connectivity results for participant: 404, mode: EC\n",
      "Successfully saved connectivity results for participant: 404, mode: EO\n",
      "Successfully saved connectivity results for participant: 406, mode: EC\n",
      "Successfully saved connectivity results for participant: 406, mode: EO\n",
      "Successfully saved connectivity results for participant: 407, mode: EC\n",
      "Successfully saved connectivity results for participant: 407, mode: EO\n",
      "Successfully saved connectivity results for participant: 408, mode: EC\n",
      "Successfully saved connectivity results for participant: 408, mode: EO\n",
      "Successfully saved connectivity results for participant: 409, mode: EC\n",
      "Successfully saved connectivity results for participant: 409, mode: EO\n",
      "Successfully saved connectivity results for participant: 410, mode: EC\n",
      "Successfully saved connectivity results for participant: 410, mode: EO\n",
      "Successfully saved connectivity results for participant: 411, mode: EC\n",
      "Successfully saved connectivity results for participant: 411, mode: EO\n",
      "Successfully saved connectivity results for participant: 412, mode: EC\n",
      "Successfully saved connectivity results for participant: 412, mode: EO\n",
      "Successfully saved connectivity results for participant: 413, mode: EC\n",
      "Successfully saved connectivity results for participant: 413, mode: EO\n",
      "Successfully saved connectivity results for participant: 414, mode: EC\n",
      "Successfully saved connectivity results for participant: 414, mode: EO\n",
      "Successfully saved connectivity results for participant: 415, mode: EC\n",
      "Successfully saved connectivity results for participant: 415, mode: EO\n",
      "Successfully saved connectivity results for participant: 416, mode: EC\n",
      "Successfully saved connectivity results for participant: 416, mode: EO\n",
      "Total time taken: 46.56 minutes\n"
     ]
    }
   ],
   "source": [
    "# Functional Connectivity Calculation (within/betweeen/aggregated-within and between)\n",
    "\n",
    "def load_thresholded_matrices(participants):\n",
    "    \"\"\"Loads thresholded matrices for all participants.\"\"\"\n",
    "    thresholded_correlation_matrices = {}\n",
    "    for participant in sorted(participants):\n",
    "        participant_dir = os.path.join(base_dir, participant)\n",
    "        for mode in [\"EC\", \"EO\"]:\n",
    "            mode_dir = os.path.join(participant_dir, mode)\n",
    "            filepath = os.path.join(mode_dir, f\"{participant}_thresholded_matrices_raw.npz\")\n",
    "            if os.path.exists(filepath):\n",
    "                data = np.load(filepath)\n",
    "                for key in data.files:\n",
    "                    try:\n",
    "                        tuple_key = tuple(map(int, key.split('_')))\n",
    "                        thresholded_correlation_matrices[(participant, mode, tuple_key)] = data[key]\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    return thresholded_correlation_matrices\n",
    "\n",
    "# Load optimal alpha and bootstrap median from JSON file\n",
    "def load_alpha_and_median(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"optimal_alpha\"], data[\"bootstrap_median\"]\n",
    "\n",
    "# Updated file path for the .json\n",
    "json_path = \"/home/cerna3/neuroconn/data/out/subjects/alpha_and_median.json\"\n",
    "optimal_alpha, bootstrap_median = load_alpha_and_median(json_path)\n",
    "\n",
    "# Start timer\n",
    "start_time_1 = time.time()\n",
    "\n",
    "# After processing all participants:\n",
    "thresholded_correlation_matrices = load_thresholded_matrices(participants_data.keys())\n",
    "\n",
    "# Load labels from the atlas \n",
    "labels = [\n",
    "    '7Networks_LH_Cont_Cing_1-lh',\n",
    "    '7Networks_LH_Cont_Par_1-lh',\n",
    "    '7Networks_LH_Cont_PFCl_1-lh',\n",
    "    '7Networks_LH_Cont_pCun_1-lh',\n",
    "    '7Networks_LH_Default_Par_1-lh',\n",
    "    '7Networks_LH_Default_Par_2-lh',\n",
    "    '7Networks_LH_Default_pCunPCC_1-lh',\n",
    "    '7Networks_LH_Default_pCunPCC_2-lh',\n",
    "    '7Networks_LH_Default_PFC_1-lh',\n",
    "    '7Networks_LH_Default_PFC_2-lh',\n",
    "    '7Networks_LH_Default_PFC_3-lh',\n",
    "    '7Networks_LH_Default_PFC_4-lh',\n",
    "    '7Networks_LH_Default_PFC_5-lh',\n",
    "    '7Networks_LH_Default_PFC_6-lh',\n",
    "    '7Networks_LH_Default_PFC_7-lh',\n",
    "    '7Networks_LH_Default_Temp_1-lh',\n",
    "    '7Networks_LH_Default_Temp_2-lh',\n",
    "    '7Networks_LH_DorsAttn_FEF_1-lh',\n",
    "    '7Networks_LH_DorsAttn_Post_1-lh',\n",
    "    '7Networks_LH_DorsAttn_Post_2-lh',\n",
    "    '7Networks_LH_DorsAttn_Post_3-lh',\n",
    "    '7Networks_LH_DorsAttn_Post_4-lh',\n",
    "    '7Networks_LH_DorsAttn_Post_5-lh',\n",
    "    '7Networks_LH_DorsAttn_Post_6-lh',\n",
    "    '7Networks_LH_DorsAttn_PrCv_1-lh',\n",
    "    '7Networks_LH_Limbic_OFC_1-lh',\n",
    "    '7Networks_LH_Limbic_TempPole_1-lh',\n",
    "    '7Networks_LH_Limbic_TempPole_2-lh',\n",
    "    '7Networks_LH_SalVentAttn_FrOperIns_1-lh',\n",
    "    '7Networks_LH_SalVentAttn_FrOperIns_2-lh',\n",
    "    '7Networks_LH_SalVentAttn_Med_1-lh',\n",
    "    '7Networks_LH_SalVentAttn_Med_2-lh',\n",
    "    '7Networks_LH_SalVentAttn_Med_3-lh',\n",
    "    '7Networks_LH_SalVentAttn_ParOper_1-lh',\n",
    "    '7Networks_LH_SalVentAttn_PFCl_1-lh',\n",
    "    '7Networks_LH_SomMot_1-lh',\n",
    "    '7Networks_LH_SomMot_2-lh',\n",
    "    '7Networks_LH_SomMot_3-lh',\n",
    "    '7Networks_LH_SomMot_4-lh',\n",
    "    '7Networks_LH_SomMot_5-lh',\n",
    "    '7Networks_LH_SomMot_6-lh',\n",
    "    '7Networks_LH_Vis_1-lh',\n",
    "    '7Networks_LH_Vis_2-lh',\n",
    "    '7Networks_LH_Vis_3-lh',\n",
    "    '7Networks_LH_Vis_4-lh',\n",
    "    '7Networks_LH_Vis_5-lh',\n",
    "    '7Networks_LH_Vis_6-lh',\n",
    "    '7Networks_LH_Vis_7-lh',\n",
    "    '7Networks_LH_Vis_8-lh',\n",
    "    '7Networks_LH_Vis_9-lh',\n",
    "    '7Networks_RH_Cont_Cing_1-rh',\n",
    "    '7Networks_RH_Cont_Par_1-rh',\n",
    "    '7Networks_RH_Cont_Par_2-rh',\n",
    "    '7Networks_RH_Cont_PFCl_1-rh',\n",
    "    '7Networks_RH_Cont_PFCl_2-rh',\n",
    "    '7Networks_RH_Cont_PFCl_3-rh',\n",
    "    '7Networks_RH_Cont_PFCl_4-rh',\n",
    "    '7Networks_RH_Cont_PFCmp_1-rh',\n",
    "    '7Networks_RH_Cont_pCun_1-rh',\n",
    "    '7Networks_RH_Default_Par_1-rh',\n",
    "    '7Networks_RH_Default_pCunPCC_1-rh',\n",
    "    '7Networks_RH_Default_pCunPCC_2-rh',\n",
    "    '7Networks_RH_Default_PFCdPFCm_1-rh',\n",
    "    '7Networks_RH_Default_PFCdPFCm_2-rh',\n",
    "    '7Networks_RH_Default_PFCdPFCm_3-rh',\n",
    "    '7Networks_RH_Default_PFCv_1-rh',\n",
    "    '7Networks_RH_Default_PFCv_2-rh',\n",
    "    '7Networks_RH_Default_Temp_1-rh',\n",
    "    '7Networks_RH_Default_Temp_2-rh',\n",
    "    '7Networks_RH_Default_Temp_3-rh',\n",
    "    '7Networks_RH_DorsAttn_FEF_1-rh',\n",
    "    '7Networks_RH_DorsAttn_Post_1-rh',\n",
    "    '7Networks_RH_DorsAttn_Post_2-rh',\n",
    "    '7Networks_RH_DorsAttn_Post_3-rh',\n",
    "    '7Networks_RH_DorsAttn_Post_4-rh',\n",
    "    '7Networks_RH_DorsAttn_Post_5-rh',\n",
    "    '7Networks_RH_DorsAttn_PrCv_1-rh',\n",
    "    '7Networks_RH_Limbic_OFC_1-rh',\n",
    "    '7Networks_RH_Limbic_TempPole_1-rh',\n",
    "    '7Networks_RH_SalVentAttn_FrOperIns_1-rh',\n",
    "    '7Networks_RH_SalVentAttn_Med_1-rh',\n",
    "    '7Networks_RH_SalVentAttn_Med_2-rh',\n",
    "    '7Networks_RH_SalVentAttn_TempOccPar_1-rh',\n",
    "    '7Networks_RH_SalVentAttn_TempOccPar_2-rh',\n",
    "    '7Networks_RH_SomMot_1-rh',\n",
    "    '7Networks_RH_SomMot_2-rh',\n",
    "    '7Networks_RH_SomMot_3-rh',\n",
    "    '7Networks_RH_SomMot_4-rh',\n",
    "    '7Networks_RH_SomMot_5-rh',\n",
    "    '7Networks_RH_SomMot_6-rh',\n",
    "    '7Networks_RH_SomMot_7-rh',\n",
    "    '7Networks_RH_SomMot_8-rh',\n",
    "    '7Networks_RH_Vis_1-rh',\n",
    "    '7Networks_RH_Vis_2-rh',\n",
    "    '7Networks_RH_Vis_3-rh',\n",
    "    '7Networks_RH_Vis_4-rh',\n",
    "    '7Networks_RH_Vis_5-rh',\n",
    "    '7Networks_RH_Vis_6-rh',\n",
    "    '7Networks_RH_Vis_7-rh',\n",
    "    '7Networks_RH_Vis_8-rh'\n",
    "]\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Initialize a dictionary to hold the network assignments\n",
    "networks = {\n",
    "    'Visual': [],\n",
    "    'Somatomotor': [],\n",
    "    'DorsalAttention': [],\n",
    "    'VentralAttention': [],\n",
    "    'Limbic': [],\n",
    "    'Frontoparietal': [],\n",
    "    'Default': []\n",
    "}\n",
    "\n",
    "def get_network_name(label_name):\n",
    "    \"\"\"Extract the network name from the label name.\"\"\"\n",
    "    if 'Vis' in label_name:\n",
    "        return 'Visual'\n",
    "    elif 'SomMot' in label_name:\n",
    "        return 'Somatomotor'\n",
    "    elif 'DorsAttn' in label_name:\n",
    "        return 'DorsalAttention'\n",
    "    elif 'SalVentAttn' in label_name or 'VentAttn' in label_name:\n",
    "        return 'VentralAttention'\n",
    "    elif 'Limbic' in label_name:\n",
    "        return 'Limbic'\n",
    "    elif 'Cont' in label_name or 'Frontoparietal' in label_name:\n",
    "        return 'Frontoparietal'\n",
    "    elif 'Default' in label_name:\n",
    "        return 'Default'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_region_name(label_name):\n",
    "    \"\"\"Extract the region name from the label name.\"\"\"\n",
    "    parts = label_name.split('-')\n",
    "    network_name = get_network_name(label_name)\n",
    "    if network_name == 'Visual' or network_name == 'Somatomotor':\n",
    "        region_info = parts[0].split('_')[2:]\n",
    "    else:\n",
    "        region_info = parts[0].split('_')[3:]\n",
    "    hemisphere = parts[1]\n",
    "    return '_'.join(region_info) + '-' + hemisphere\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    network_name = get_network_name(label)\n",
    "    if network_name:\n",
    "        region_name = get_region_name(label)\n",
    "        networks[network_name].append((i, region_name))\n",
    "\n",
    "# Define calculate_network_connectivity function \n",
    "def calculate_network_connectivity(thresholded_correlation_matrices, networks):\n",
    "    \"\"\"\n",
    "    Calculate connectivity within and between networks using thresholded correlation matrices.\n",
    "\n",
    "    Args:\n",
    "        thresholded_correlation_matrices (dict): Dictionary containing thresholded correlation matrices \n",
    "            for each participant, mode, and state-window combination.\n",
    "        networks (dict): Dictionary mapping network names to lists of region indices and names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries: within_network_connectivity and \n",
    "            between_network_connectivity.\n",
    "            - within_network_connectivity: Connectivity values within each network for each state-window.\n",
    "            - between_network_connectivity: Connectivity values between different networks for each state-window.\n",
    "    \"\"\"\n",
    "    print(\"Starting within/between network connectivity calculations...\")\n",
    "    within_network_connectivity = {}\n",
    "    between_network_connectivity = {}\n",
    "    networks_indices = {k: [r[0] for r in v] for k, v in networks.items()} \n",
    "\n",
    "    participants_processed = set()\n",
    "    participant_count = len(participants_data)\n",
    "    current_count = 0\n",
    "\n",
    "    for (participant, mode, window), corr_matrix in sorted(thresholded_correlation_matrices.items()):\n",
    "        # Initialize list for within-network connectivity\n",
    "        within_network_connectivity[(participant, mode, window)] = []  \n",
    "        # Initialize list for between-network connectivity\n",
    "        between_network_connectivity[(participant, mode, window)] = []  \n",
    "\n",
    "        for network_name, regions in networks.items():\n",
    "            # Calculate within-network connectivity\n",
    "            network_corr_matrix = corr_matrix[np.ix_(networks_indices[network_name], networks_indices[network_name])] \n",
    "            upper_tri_indices = np.triu_indices_from(network_corr_matrix, k=1)\n",
    "            for i, j in zip(*upper_tri_indices):  \n",
    "                region1 = regions[i][1]  \n",
    "                region2 = regions[j][1] \n",
    "                corr_value = network_corr_matrix[i, j]  \n",
    "                within_network_connectivity[(participant, mode, window)].append(f\"[{network_name}]: {region1} - {corr_value:.2f} - {region2}\")  \n",
    "\n",
    "        # Calculate between-network connectivity\n",
    "        for net1, net2 in itertools.combinations(networks.keys(), 2): \n",
    "            regions1, regions2 = networks[net1], networks[net2]\n",
    "            between_corr_matrix = corr_matrix[np.ix_(networks_indices[net1], networks_indices[net2])] \n",
    "            for i, j in itertools.product(range(len(regions1)), range(len(regions2))):\n",
    "                region1 = regions1[i][1] \n",
    "                region2 = regions2[j][1] \n",
    "                corr_value = between_corr_matrix[i, j] \n",
    "                between_network_connectivity[(participant, mode, window)].append(f\"[{net1}, {net2}]: {region1} - {corr_value:.2f} - {region2}\")  \n",
    "\n",
    "        if participant not in participants_processed:\n",
    "            ec_processed = (participant, \"EC\", window) in thresholded_correlation_matrices\n",
    "            eo_processed = (participant, \"EO\", window) in thresholded_correlation_matrices\n",
    "\n",
    "            if ec_processed and eo_processed:\n",
    "                current_count += 1\n",
    "                participants_processed.add(participant)\n",
    "                sys.stdout.write(f\"\\rWithin/Between network connectivity calculated for participant: {participant} for modes EC and EO ({current_count}/{participant_count})\")\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                print(f\"\\rError processing participant: {participant}. Mode EC or EO not successfully processed.\")\n",
    "                \n",
    "\n",
    "    print(\"\\nCompleted calculation of network connectivity for all participants.\")\n",
    "    return within_network_connectivity, between_network_connectivity\n",
    "\n",
    "# Calculate within and between network connectivity\n",
    "start_time = time.time()\n",
    "within_network_conn_values, between_network_conn_values = calculate_network_connectivity(thresholded_correlation_matrices, networks)\n",
    "print(f\"Total time taken for calculation: {(time.time() - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "def aggregate_network_connectivity(thresholded_correlation_matrices, networks, median_optimal_states):\n",
    "    \"\"\"\n",
    "    Aggregate network connectivity measures for each state by averaging across windows.\n",
    "    NOTE: Add back error handling if need to troubleshoot\n",
    "\n",
    "    Args:\n",
    "        thresholded_correlation_matrices (dict): Dictionary containing thresholded correlation matrices \n",
    "            for each participant, mode, and state-window combination. Keys are tuples\n",
    "            in the form (participant, mode, (state, window_number)).\n",
    "        networks (dict): Dictionary mapping network names to lists of region indices and names.\n",
    "        median_optimal_states (int): The optimal number of states identified.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries: aggregated_within_network_connectivity and \n",
    "            aggregated_between_network_connectivity.\n",
    "            - aggregated_within_network_connectivity: Average connectivity within each network for each state.\n",
    "            - aggregated_between_network_connectivity: Average connectivity between different networks for each state.\n",
    "    \"\"\"\n",
    "    print(\"Starting aggregation of network connectivity...\")\n",
    "    aggregated_within_network_connectivity = {}\n",
    "    aggregated_between_network_connectivity = {}\n",
    "    networks_indices = {k: [r[0] for r in v] for k, v in networks.items()}\n",
    "\n",
    "    participants_processed = set()\n",
    "    participant_count = len(participants_data)\n",
    "    current_count = 0\n",
    "    error_participants = set()  # Use a set to avoid duplicates\n",
    "    max_message_length = 0\n",
    "\n",
    "    # Convert median_optimal_states to an integer\n",
    "    median_optimal_states = int(median_optimal_states)\n",
    "\n",
    "    # Initialize the connectivity dictionaries for each state\n",
    "    for state in range(median_optimal_states):\n",
    "        aggregated_within_network_connectivity[state] = {network: [] for network in networks.keys()}\n",
    "        aggregated_between_network_connectivity[state] = {(net1, net2): [] for net1, net2 in itertools.combinations(networks.keys(), 2)}\n",
    "\n",
    "    # Aggregate connectivity measures for each state\n",
    "    for (participant, mode, (state, window_number)), corr_matrix in sorted(thresholded_correlation_matrices.items()):\n",
    "        # Ensure the state key exists in the dictionaries\n",
    "        if state not in aggregated_within_network_connectivity:\n",
    "            aggregated_within_network_connectivity[state] = {network: [] for network in networks.keys()}\n",
    "        if state not in aggregated_between_network_connectivity:\n",
    "            aggregated_between_network_connectivity[state] = {(net1, net2): [] for net1, net2 in itertools.combinations(networks.keys(), 2)}\n",
    "\n",
    "        # Within-network connectivity\n",
    "        for network_name, regions in networks.items():\n",
    "            network_corr_matrix = corr_matrix[np.ix_(networks_indices[network_name], networks_indices[network_name])]\n",
    "            upper_tri_indices = np.triu_indices_from(network_corr_matrix, k=1)\n",
    "            mean_corr = np.mean(network_corr_matrix[upper_tri_indices])\n",
    "            if not np.isnan(mean_corr):  # Check if mean_corr is not NaN\n",
    "                aggregated_within_network_connectivity[state][network_name].append(mean_corr)\n",
    "\n",
    "        # Between-network connectivity\n",
    "        for net1, net2 in itertools.combinations(networks.keys(), 2):\n",
    "            regions1 = networks_indices[net1]\n",
    "            regions2 = networks_indices[net2]\n",
    "            between_corr_matrix = corr_matrix[np.ix_(regions1, regions2)]\n",
    "            mean_corr = np.mean(between_corr_matrix)\n",
    "            if not np.isnan(mean_corr):  # Check if mean_corr is not NaN\n",
    "                aggregated_between_network_connectivity[state][(net1, net2)].append(mean_corr)\n",
    "                \n",
    "        # except KeyError as e:\n",
    "        #     error_message = f\"Error processing participant: {participant}, mode: {mode}, state: {state}. Reason: KeyError: {e.args[0]} not found\"\n",
    "        #     sys.stdout.write('\\r' + ' ' * max_message_length + '\\r')  # Clear the line\n",
    "        #     sys.stdout.write(f\"\\r{error_message}\")\n",
    "        #     sys.stdout.flush()\n",
    "        #     error_participants.add(participant)\n",
    "        #     max_message_length = max(max_message_length, len(error_message))\n",
    "        #     continue  # Skip the rest of the loop for this participant/mode/state\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     error_message = f\"Error processing participant: {participant}, mode: {mode}, state: {state}. Reason: {type(e).__name__}: {e}\"\n",
    "        #     sys.stdout.write('\\r' + ' ' * max_message_length + '\\r')  # Clear the line\n",
    "        #     sys.stdout.write(f\"\\r{error_message}\")\n",
    "        #     sys.stdout.flush()\n",
    "        #     error_participants.add(participant)\n",
    "        #     max_message_length = max(max_message_length, len(error_message))\n",
    "        #     continue  # Skip the rest of the loop for this participant/mode/state\n",
    "\n",
    "        if participant not in participants_processed:\n",
    "            ec_processed = any((participant, \"EC\", (state, wn)) in thresholded_correlation_matrices for wn in range(window_number + 1))\n",
    "            eo_processed = any((participant, \"EO\", (state, wn)) in thresholded_correlation_matrices for wn in range(window_number + 1))\n",
    "\n",
    "            if ec_processed and eo_processed:\n",
    "                current_count += 1\n",
    "                participants_processed.add(participant)\n",
    "                elapsed_time_participant = time.time() - start_time\n",
    "                progress_message = f\"Aggregated network connectivity calculated for participant: {participant} for modes EC and EO ({current_count}/{participant_count}).\"\n",
    "                sys.stdout.write('\\r' + ' ' * max_message_length + '\\r')  # Clear the line\n",
    "                sys.stdout.write(f\"\\r{progress_message}\")\n",
    "                sys.stdout.flush()\n",
    "                max_message_length = max(max_message_length, len(progress_message))\n",
    "                \n",
    "            # else:\n",
    "            #     error_message = f\"Error processing participant: {participant}. Mode EC or EO not successfully processed for state {state}.\"\n",
    "            #     sys.stdout.write('\\r' + ' ' * max_message_length + '\\r')  # Clear the line\n",
    "            #     sys.stdout.write(f\"\\r{error_message}\")\n",
    "            #     sys.stdout.flush()\n",
    "            #     error_participants.add(participant)\n",
    "            #     max_message_length = max(max_message_length, len(error_message))\n",
    "\n",
    "    # Calculate average connectivity for each state\n",
    "    for state in range(median_optimal_states):\n",
    "        for network in aggregated_within_network_connectivity[state]:\n",
    "            if aggregated_within_network_connectivity[state][network]:  # Check if list is not empty\n",
    "                aggregated_within_network_connectivity[state][network] = np.mean(aggregated_within_network_connectivity[state][network])\n",
    "            else:\n",
    "                aggregated_within_network_connectivity[state][network] = np.nan  # Assign NaN if list is empty\n",
    "\n",
    "        for net_pair in aggregated_between_network_connectivity[state]:\n",
    "            if aggregated_between_network_connectivity[state][net_pair]:  # Check if list is not empty\n",
    "                aggregated_between_network_connectivity[state][net_pair] = np.mean(aggregated_between_network_connectivity[state][net_pair])\n",
    "            else:\n",
    "                aggregated_between_network_connectivity[state][net_pair] = np.nan  # Assign NaN if list is empty\n",
    "\n",
    "    print(\"\\nCompleted aggregation of network connectivity for all participants.\")\n",
    "   # if error_participants:\n",
    "   #     print(f\"\\nParticipants with errors: {', '.join(sorted(error_participants))}\")\n",
    "\n",
    "    return aggregated_within_network_connectivity, aggregated_between_network_connectivity\n",
    "\n",
    "# Call the function and measure execution time\n",
    "start_time = time.time()\n",
    "aggregated_within_conn, aggregated_between_conn = aggregate_network_connectivity(thresholded_correlation_matrices, networks, median_optimal_state)\n",
    "print(f\"Total time taken for aggregation: {(time.time() - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "def save_connectivity_results(participant, mode, within_network_conn_values, \n",
    "                              between_network_conn_values, aggregated_within_conn, \n",
    "                              aggregated_between_conn):\n",
    "    \"\"\"Saves connectivity results to separate files within the participant's folder.\"\"\"\n",
    "    participant_dir = os.path.join(base_dir, participant)\n",
    "    mode_dir = os.path.join(participant_dir, mode)\n",
    "    os.makedirs(mode_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    try:\n",
    "        # Save network assignment to a file\n",
    "        network_assignment_file = os.path.join(mode_dir, f\"{participant}_{mode}_network_assignment.npy\")\n",
    "        np.save(network_assignment_file, networks)\n",
    "\n",
    "        # Save within-network connectivity values\n",
    "        within_conn_file = os.path.join(mode_dir, f\"{participant}_{mode}_within_network_conn_raw.npz\")\n",
    "        within_arrays_to_save = {}\n",
    "        for key, value in within_network_conn_values.items():\n",
    "            key_str = \"_\".join(map(str, key))\n",
    "            within_arrays_to_save[key_str] = value\n",
    "        np.savez(within_conn_file, **within_arrays_to_save)\n",
    "\n",
    "        # Save between-network connectivity values\n",
    "        between_conn_file = os.path.join(mode_dir, f\"{participant}_{mode}_between_network_conn_raw.npz\")\n",
    "        between_arrays_to_save = {}\n",
    "        for key, value in between_network_conn_values.items():\n",
    "            key_str = \"_\".join(map(str, key))\n",
    "            between_arrays_to_save[key_str] = value\n",
    "        np.savez(between_conn_file, **between_arrays_to_save)\n",
    "\n",
    "        # Save aggregated connectivity measures\n",
    "        aggregated_conn_file = os.path.join(mode_dir, f\"{participant}_{mode}_aggregated_conn_raw.npz\")\n",
    "        np.savez(aggregated_conn_file, within_conn=aggregated_within_conn, \n",
    "                 between_conn=aggregated_between_conn)\n",
    "        \n",
    "        print(f\"Successfully saved connectivity results for participant: {participant}, mode: {mode}\") \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving results for participant: {participant}, mode: {mode}. Reason: {str(e)}\")\n",
    "    \n",
    "\n",
    "# Call the save function for each participant and mode\n",
    "for participant, results in participants_data.items():\n",
    "    for mode, data in results.items():\n",
    "        if 'correlation_matrices' in data and 'thresholded_matrices' in data:\n",
    "            # Extract the connectivity values for the current participant and mode\n",
    "            within_conn_values = {\n",
    "                key: value for key, value in within_network_conn_values.items() if key[0] == participant and key[1] == mode\n",
    "            }\n",
    "            between_conn_values = {\n",
    "                key: value for key, value in between_network_conn_values.items() if key[0] == participant and key[1] == mode\n",
    "            }\n",
    "            # Save the results\n",
    "            save_connectivity_results(participant, mode, within_conn_values,\n",
    "                                      between_conn_values, aggregated_within_conn,\n",
    "                                      aggregated_between_conn)\n",
    "            \n",
    "end_time = time.time()  \n",
    "elapsed_time_minutes = (end_time - start_time_1) / 60\n",
    "print(f\"Total time taken: {elapsed_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All correlations in thresholded are within -1 to +1 bounds\n",
      "All correlations in within network are within -1 to +1 bounds\n",
      "All correlations in between network are within -1 to +1 bounds\n",
      "All correlations in aggregated within are within -1 to +1 bounds\n",
      "All correlations in aggregated between are within -1 to +1 bounds\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK!\n",
    "\n",
    "def load_all_matrices(participants, base_dir):\n",
    "    \"\"\"Loads all relevant matrices for analysis including thresholded, within-network, between-network, and aggregated connectivity matrices.\"\"\"\n",
    "    matrices = {\n",
    "        'thresholded': {},\n",
    "        'within_network': {},\n",
    "        'between_network': {},\n",
    "        'aggregated_within': {},\n",
    "        'aggregated_between': {}\n",
    "    }\n",
    "\n",
    "    for participant in participants:\n",
    "        participant_dir = os.path.join(base_dir, participant)\n",
    "        for mode in [\"EC\", \"EO\"]:\n",
    "            mode_dir = os.path.join(participant_dir, mode)\n",
    "            \n",
    "            # Load thresholded matrices\n",
    "            thresholded_path = os.path.join(mode_dir, f\"{participant}_thresholded_matrices_raw.npz\")\n",
    "            if os.path.exists(thresholded_path):\n",
    "                data = np.load(thresholded_path, allow_pickle=True)\n",
    "                for key in data.files:\n",
    "                    try:\n",
    "                        tuple_key = (participant, mode, tuple(map(int, key.split('_'))))\n",
    "                        matrices['thresholded'][tuple_key] = data[key]\n",
    "                    except ValueError:\n",
    "                        continue  # Skip keys that don't convert to integers\n",
    "            \n",
    "            # Load within-network connectivity\n",
    "            within_path = os.path.join(mode_dir, f\"{participant}_{mode}_within_network_conn_raw.npz\")\n",
    "            if os.path.exists(within_path):\n",
    "                data = np.load(within_path, allow_pickle=True)\n",
    "                for key in data.files:\n",
    "                    matrices['within_network'][(participant, mode, key)] = data[key]\n",
    "            \n",
    "            # Load between-network connectivity\n",
    "            between_path = os.path.join(mode_dir, f\"{participant}_{mode}_between_network_conn_raw.npz\")\n",
    "            if os.path.exists(between_path):\n",
    "                data = np.load(between_path, allow_pickle=True)\n",
    "                for key in data.files:\n",
    "                    matrices['between_network'][(participant, mode, key)] = data[key]\n",
    "            \n",
    "            # Load aggregated connectivity\n",
    "            aggregated_path = os.path.join(mode_dir, f\"{participant}_{mode}_aggregated_conn_raw.npz\")\n",
    "            if os.path.exists(aggregated_path):\n",
    "                data = np.load(aggregated_path, allow_pickle=True)\n",
    "                matrices['aggregated_within'][(participant, mode, 'within')] = data['within_conn']\n",
    "                matrices['aggregated_between'][(participant, mode, 'between')] = data['between_conn']\n",
    "            else:\n",
    "                print(f\"File missing for participant {participant}, mode {mode}\")\n",
    "\n",
    "    return matrices\n",
    "\n",
    "def check_bounds(matrix):\n",
    "    \"\"\"Check if any values in the matrix are out of the expected correlation bounds [-1, 1].\"\"\"\n",
    "    return np.any((matrix < -1) | (matrix > 1))\n",
    "\n",
    "def analyze_and_print_results(matrices):\n",
    "    \"\"\"Analyzes newly created matrices and prints whether all correlations are within bounds or not.\"\"\"\n",
    "    categories_to_check = ['thresholded', 'within_network', 'between_network', 'aggregated_within', 'aggregated_between']\n",
    "    \n",
    "    for category in categories_to_check:\n",
    "        out_of_bounds_found = False\n",
    "        for matrix in matrices[category].values():\n",
    "            if isinstance(matrix, np.ndarray) and matrix.ndim == 2:\n",
    "                if check_bounds(matrix):\n",
    "                    out_of_bounds_found = True\n",
    "                    break\n",
    "        if out_of_bounds_found:\n",
    "            print(f\"Some correlations in {category.replace('_', ' ')} are out of bounds\")\n",
    "        else:\n",
    "            print(f\"All correlations in {category.replace('_', ' ')} are within -1 to +1 bounds\")\n",
    "            \n",
    "# Main code to load matrices and perform analysis\n",
    "base_dir = '/home/cerna3/neuroconn/data/out/subjects/'\n",
    "participants = ['101', '102', '103', '104', '105', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120',\n",
    "                '202', '205', '206', '207', '208', '209', '210', '211', '214', '215', '216', '217', '218', '219', '221',\n",
    "                '401', '402', '403', '404', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416']\n",
    "matrices = load_all_matrices(participants, base_dir)\n",
    "\n",
    "# Analyze and print results\n",
    "analyze_and_print_results(matrices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-hmm_env]",
   "language": "python",
   "name": "conda-env-anaconda3-hmm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
