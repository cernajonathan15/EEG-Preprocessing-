{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 540 transition probability records.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 540 entries, 0 to 1078\n",
      "Data columns (total 68 columns):\n",
      " #   Column                                                                       Non-Null Count  Dtype  \n",
      "---  ------                                                                       --------------  -----  \n",
      " 0   ID                                                                           540 non-null    int64  \n",
      " 1   Group                                                                        540 non-null    object \n",
      " 2   Mode                                                                         540 non-null    object \n",
      " 3   State                                                                        540 non-null    int64  \n",
      " 4   Default_w_mean                                                               540 non-null    float64\n",
      " 5   DorsalAttention_w_mean                                                       540 non-null    float64\n",
      " 6   Frontoparietal_w_mean                                                        540 non-null    float64\n",
      " 7   Limbic_w_mean                                                                540 non-null    float64\n",
      " 8   Somatomotor_w_mean                                                           540 non-null    float64\n",
      " 9   VentralAttention_w_mean                                                      540 non-null    float64\n",
      " 10  Visual_w_mean                                                                540 non-null    float64\n",
      " 11  Default_w_tran_mag                                                           540 non-null    float64\n",
      " 12  DorsalAttention_w_tran_mag                                                   540 non-null    float64\n",
      " 13  Frontoparietal_w_tran_mag                                                    540 non-null    float64\n",
      " 14  Limbic_w_tran_mag                                                            540 non-null    float64\n",
      " 15  Somatomotor_w_tran_mag                                                       540 non-null    float64\n",
      " 16  VentralAttention_w_tran_mag                                                  540 non-null    float64\n",
      " 17  Visual_w_tran_mag                                                            540 non-null    float64\n",
      " 18  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,D,e,f,a,u,l,t_b_mean                        540 non-null    float64\n",
      " 19  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_mean          540 non-null    float64\n",
      " 20  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,L,i,m,b,i,c_b_mean                          540 non-null    float64\n",
      " 21  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n_b_mean      540 non-null    float64\n",
      " 22  F,r,o,n,t,o,p,a,r,i,e,t,a,l,,_,D,e,f,a,u,l,t_b_mean                          540 non-null    float64\n",
      " 23  L,i,m,b,i,c,,_,D,e,f,a,u,l,t_b_mean                                          540 non-null    float64\n",
      " 24  L,i,m,b,i,c,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_mean                            540 non-null    float64\n",
      " 25  S,o,m,a,t,o,m,o,t,o,r,,_,D,e,f,a,u,l,t_b_mean                                540 non-null    float64\n",
      " 26  S,o,m,a,t,o,m,o,t,o,r,,_,D,o,r,s,a,l,A,t,t,e,n,t,i,o,n_b_mean                540 non-null    float64\n",
      " 27  S,o,m,a,t,o,m,o,t,o,r,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_mean                  540 non-null    float64\n",
      " 28  S,o,m,a,t,o,m,o,t,o,r,,_,L,i,m,b,i,c_b_mean                                  540 non-null    float64\n",
      " 29  S,o,m,a,t,o,m,o,t,o,r,,_,V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n_b_mean              540 non-null    float64\n",
      " 30  V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n,,_,D,e,f,a,u,l,t_b_mean                      540 non-null    float64\n",
      " 31  V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_mean        540 non-null    float64\n",
      " 32  V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n,,_,L,i,m,b,i,c_b_mean                        540 non-null    float64\n",
      " 33  V,i,s,u,a,l,,_,D,e,f,a,u,l,t_b_mean                                          540 non-null    float64\n",
      " 34  V,i,s,u,a,l,,_,D,o,r,s,a,l,A,t,t,e,n,t,i,o,n_b_mean                          540 non-null    float64\n",
      " 35  V,i,s,u,a,l,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_mean                            540 non-null    float64\n",
      " 36  V,i,s,u,a,l,,_,L,i,m,b,i,c_b_mean                                            540 non-null    float64\n",
      " 37  V,i,s,u,a,l,,_,S,o,m,a,t,o,m,o,t,o,r_b_mean                                  540 non-null    float64\n",
      " 38  V,i,s,u,a,l,,_,V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n_b_mean                        540 non-null    float64\n",
      " 39  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,D,e,f,a,u,l,t_b_tran_mag                    540 non-null    float64\n",
      " 40  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_tran_mag      540 non-null    float64\n",
      " 41  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,L,i,m,b,i,c_b_tran_mag                      540 non-null    float64\n",
      " 42  D,o,r,s,a,l,A,t,t,e,n,t,i,o,n,,_,V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n_b_tran_mag  540 non-null    float64\n",
      " 43  F,r,o,n,t,o,p,a,r,i,e,t,a,l,,_,D,e,f,a,u,l,t_b_tran_mag                      540 non-null    float64\n",
      " 44  L,i,m,b,i,c,,_,D,e,f,a,u,l,t_b_tran_mag                                      540 non-null    float64\n",
      " 45  L,i,m,b,i,c,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_tran_mag                        540 non-null    float64\n",
      " 46  S,o,m,a,t,o,m,o,t,o,r,,_,D,e,f,a,u,l,t_b_tran_mag                            540 non-null    float64\n",
      " 47  S,o,m,a,t,o,m,o,t,o,r,,_,D,o,r,s,a,l,A,t,t,e,n,t,i,o,n_b_tran_mag            540 non-null    float64\n",
      " 48  S,o,m,a,t,o,m,o,t,o,r,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_tran_mag              540 non-null    float64\n",
      " 49  S,o,m,a,t,o,m,o,t,o,r,,_,L,i,m,b,i,c_b_tran_mag                              540 non-null    float64\n",
      " 50  S,o,m,a,t,o,m,o,t,o,r,,_,V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n_b_tran_mag          540 non-null    float64\n",
      " 51  V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n,,_,D,e,f,a,u,l,t_b_tran_mag                  540 non-null    float64\n",
      " 52  V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_tran_mag    540 non-null    float64\n",
      " 53  V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n,,_,L,i,m,b,i,c_b_tran_mag                    540 non-null    float64\n",
      " 54  V,i,s,u,a,l,,_,D,e,f,a,u,l,t_b_tran_mag                                      540 non-null    float64\n",
      " 55  V,i,s,u,a,l,,_,D,o,r,s,a,l,A,t,t,e,n,t,i,o,n_b_tran_mag                      540 non-null    float64\n",
      " 56  V,i,s,u,a,l,,_,F,r,o,n,t,o,p,a,r,i,e,t,a,l_b_tran_mag                        540 non-null    float64\n",
      " 57  V,i,s,u,a,l,,_,L,i,m,b,i,c_b_tran_mag                                        540 non-null    float64\n",
      " 58  V,i,s,u,a,l,,_,S,o,m,a,t,o,m,o,t,o,r_b_tran_mag                              540 non-null    float64\n",
      " 59  V,i,s,u,a,l,,_,V,e,n,t,r,a,l,A,t,t,e,n,t,i,o,n_b_tran_mag                    540 non-null    float64\n",
      " 60  Mean_Lifetime                                                                540 non-null    float64\n",
      " 61  Fractional_Occupancy                                                         540 non-null    float64\n",
      " 62  Mean_Interval_Length                                                         540 non-null    float64\n",
      " 63  Transition_Probabilities_PCA                                                 540 non-null    float64\n",
      " 64  Transition_Probabilities_Entropy                                             540 non-null    float64\n",
      " 65  final_score_miniBEST                                                         540 non-null    float64\n",
      " 66  Practice                                                                     540 non-null    object \n",
      " 67  Age                                                                          540 non-null    int64  \n",
      "dtypes: float64(62), int64(3), object(3)\n",
      "memory usage: 291.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Dataset creation for post-hoc comparisons \n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/cerna3/neuroconn/data analyses/final_combined_data_compressed.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Function to parse connectivity values, handling within/between and EC/EO\n",
    "def parse_connectivity(data, key, suffix, is_between=False):\n",
    "    records = []\n",
    "    for idx, row in data.iterrows():\n",
    "        group = row['Group']\n",
    "        mode = row['Mode']\n",
    "        conn_dict = eval(row[key])\n",
    "\n",
    "        for networks, states in conn_dict.items():\n",
    "            if is_between:\n",
    "                networks = ','.join(networks)  # Combine network pair into single string\n",
    "            for state, value in states.items():\n",
    "                records.append({\n",
    "                    'ID': row['ID'],\n",
    "                    'Group': group,\n",
    "                    'Mode': mode,\n",
    "                    'Network' if not is_between else 'Network_Pair': networks,\n",
    "                    'State': state,\n",
    "                    'Value': value\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Function to parse transition magnitudes\n",
    "def parse_transition_magnitudes(data, key, suffix, is_between=False):\n",
    "    records = []\n",
    "    for idx, row in data.iterrows():\n",
    "        group = row['Group']\n",
    "        mode = row['Mode']\n",
    "        tran_mag_dict = eval(row[key])\n",
    "        for networks, states in tran_mag_dict.items():\n",
    "            if is_between:\n",
    "                networks = ','.join(networks) \n",
    "            for state, value in states.items():\n",
    "                if isinstance(value, list):\n",
    "                    value = value[0] if value else None\n",
    "                records.append({\n",
    "                    'ID': row['ID'],\n",
    "                    'Group': group,\n",
    "                    'Mode': mode,\n",
    "                    'Network' if not is_between else 'Network_Pair': networks,\n",
    "                    'State': state,\n",
    "                    'Value': value\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def add_temporal_variables(data, temporal_vars, var_name):\n",
    "    records = []\n",
    "    for idx, row in data.iterrows():\n",
    "        mode = row['Mode']\n",
    "        var_values = temporal_vars[idx]\n",
    "        \n",
    "        if mode == 'EC' and len(var_values) >= 7:\n",
    "            for state, value in enumerate(var_values[:7]):\n",
    "                records.append({\n",
    "                    'ID': row['ID'],\n",
    "                    'Group': row['Group'],\n",
    "                    'Mode': mode,\n",
    "                    'State': state,\n",
    "                    f'{var_name}': value\n",
    "                })\n",
    "        elif mode == 'EO' and len(var_values) >= 5:\n",
    "            for state, value in enumerate(var_values[:5]):\n",
    "                records.append({\n",
    "                    'ID': row['ID'],\n",
    "                    'Group': row['Group'],\n",
    "                    'Mode': mode,\n",
    "                    'State': state,\n",
    "                    f'{var_name}': value\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def add_transition_probabilities(data, transition_probs):\n",
    "    records = []\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        mode = row['Mode']\n",
    "        id = row['ID']\n",
    "        group = row['Group']\n",
    "        trans_probs = transition_probs[idx]\n",
    "        \n",
    "        if not isinstance(trans_probs, list) or not trans_probs:\n",
    "            print(f\"Warning: Unexpected format for transition probabilities for ID {id}, Mode {mode}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract the inner list\n",
    "        if len(trans_probs) == 1 and isinstance(trans_probs[0], list):\n",
    "            trans_probs = trans_probs[0]\n",
    "        \n",
    "        matrix_size = 7 if mode == 'EC' else 5\n",
    "        if len(trans_probs) != matrix_size * matrix_size:\n",
    "            print(f\"Warning: Insufficient data for ID {id}, Mode {mode}. Length: {len(trans_probs)}\")\n",
    "            continue\n",
    "        \n",
    "        # Reshape into matrix\n",
    "        trans_probs_matrix = np.array(trans_probs).reshape(matrix_size, matrix_size)\n",
    "        \n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=1)\n",
    "        pca_result = pca.fit_transform(trans_probs_matrix)\n",
    "        \n",
    "        # Calculate entropy for each row (state)\n",
    "        entropies = [entropy(row) for row in trans_probs_matrix]\n",
    "        \n",
    "        for state in range(matrix_size):\n",
    "            records.append({\n",
    "                'ID': id,\n",
    "                'Group': group,\n",
    "                'Mode': mode,\n",
    "                'State': state,\n",
    "                'Transition_Probabilities_PCA': pca_result[state][0],\n",
    "                'Transition_Probabilities_Entropy': entropies[state]\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(records)\n",
    "    if result_df.empty:\n",
    "        print(\"Warning: No transition probabilities were processed.\")\n",
    "    else:\n",
    "        print(f\"Processed {len(result_df)} transition probability records.\")\n",
    "    return result_df\n",
    "\n",
    "# --- Data extraction and parsing ---\n",
    "within_conn_mean_df = parse_connectivity(data, 'Within_Network_Conn_Mean', '_w_mean')\n",
    "within_tran_mag_df = parse_transition_magnitudes(data, 'Within_Network_Transition_Magnitudes', '_w_tran_mag')\n",
    "between_conn_mean_df = parse_connectivity(data, 'Between_Network_Conn_Mean', '_b_mean', is_between=True)\n",
    "between_tran_mag_df = parse_transition_magnitudes(data, 'Between_Network_Transition_Magnitudes', '_b_tran_mag', is_between=True)\n",
    "\n",
    "# --- Temporal variable extraction and processing ---\n",
    "def format_temporal_var(column, is_transition_prob=False):\n",
    "    def format_single_value(x):\n",
    "        if is_transition_prob:\n",
    "            # Remove all brackets and split by spaces\n",
    "            values = x.replace('[', '').replace(']', '').split()\n",
    "            return [float(val) for val in values if val.replace('.', '').isdigit()]\n",
    "        else:\n",
    "            # For other temporal variables, keep the existing logic\n",
    "            values = x.strip('[]').split()\n",
    "            return [float(val) for val in values if val.replace('.', '').isdigit()]\n",
    "    \n",
    "    return column.apply(format_single_value)\n",
    "\n",
    "# Modify the temporal_vars extraction:\n",
    "temporal_vars = {\n",
    "    'Transition_Probabilities': format_temporal_var(data['Transition_Probabilities'], is_transition_prob=True).tolist(),\n",
    "    'Mean_Lifetime': format_temporal_var(data['Mean_Lifetime']).tolist(),\n",
    "    'Fractional_Occupancy': format_temporal_var(data['Fractional_Occupancy']).tolist(),\n",
    "    'Mean_Interval_Length': format_temporal_var(data['Mean_Interval_Length']).tolist(),\n",
    "}\n",
    "\n",
    "# Create temporal dataframes\n",
    "mean_lifetime_df = add_temporal_variables(data, temporal_vars['Mean_Lifetime'], 'Mean_Lifetime')\n",
    "fractional_occupancy_df = add_temporal_variables(data, temporal_vars['Fractional_Occupancy'], 'Fractional_Occupancy')\n",
    "mean_interval_length_df = add_temporal_variables(data, temporal_vars['Mean_Interval_Length'], 'Mean_Interval_Length')\n",
    "transition_probabilities_df = add_transition_probabilities(data, temporal_vars['Transition_Probabilities'])\n",
    "\n",
    "# Combine all temporal dataframes\n",
    "temporal_df = mean_lifetime_df.merge(fractional_occupancy_df, on=['ID', 'Group', 'Mode', 'State'], how='outer')\n",
    "temporal_df = temporal_df.merge(mean_interval_length_df, on=['ID', 'Group', 'Mode', 'State'], how='outer')\n",
    "temporal_df = temporal_df.merge(transition_probabilities_df, on=['ID', 'Group', 'Mode', 'State'], how='outer')\n",
    "\n",
    "def remove_temporal_duplicates(df):\n",
    "    temporal_vars = ['Mean_Lifetime', 'Fractional_Occupancy', 'Mean_Interval_Length', \n",
    "                     'Transition_Probabilities_PCA', 'Transition_Probabilities_Entropy']\n",
    "    \n",
    "    for var in temporal_vars:\n",
    "        if var not in df.columns:\n",
    "            print(f\"Column {var} not found in DataFrame. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Sort the dataframe by ID, Group, Mode, State, and the temporal variable\n",
    "        df = df.sort_values(['ID', 'Group', 'Mode', 'State', var])\n",
    "        \n",
    "        # Remove duplicates, keeping the first occurrence\n",
    "        df = df.drop_duplicates(subset=['ID', 'Group', 'Mode', 'State', var], keep='first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Pivot spatial data to wide format for easier analysis ---\n",
    "def pivot_data(df, suffix):\n",
    "    if suffix in ['_w_mean', '_w_tran_mag']:\n",
    "        # For within-network data\n",
    "        pivoted = df.pivot_table(\n",
    "            index=['ID', 'Group', 'Mode', 'State'],\n",
    "            columns='Network',\n",
    "            values='Value'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Rename columns\n",
    "        pivoted.columns.name = None\n",
    "        pivoted.rename(columns={col: f\"{col}{suffix}\" for col in pivoted.columns if col not in ['ID', 'Group', 'Mode', 'State']}, inplace=True)\n",
    "    else:\n",
    "        # For between-network data\n",
    "        pivoted = df.pivot_table(\n",
    "            index=['ID', 'Group', 'Mode', 'State'],\n",
    "            columns='Network_Pair',\n",
    "            values='Value'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Rename columns, replacing comma with underscore\n",
    "        pivoted.columns.name = None\n",
    "        pivoted.rename(columns={col: f\"{col.replace(', ', '_')}{suffix}\" for col in pivoted.columns if col not in ['ID', 'Group', 'Mode', 'State']}, inplace=True)\n",
    "    \n",
    "    return pivoted\n",
    "\n",
    "# --- Pivot spatial variables ---\n",
    "within_conn_mean_pivot = pivot_data(within_conn_mean_df, '_w_mean')\n",
    "within_tran_mag_pivot = pivot_data(within_tran_mag_df, '_w_tran_mag')\n",
    "between_conn_mean_pivot = pivot_data(between_conn_mean_df, '_b_mean')\n",
    "between_tran_mag_pivot = pivot_data(between_tran_mag_df, '_b_tran_mag')\n",
    "\n",
    "# Merge spatial dataframes\n",
    "spatial_df = (\n",
    "    within_conn_mean_pivot\n",
    "    .merge(within_tran_mag_pivot, on=['ID', 'Group', 'Mode', 'State'])\n",
    "    .merge(between_conn_mean_pivot, on=['ID', 'Group', 'Mode', 'State'])\n",
    "    .merge(between_tran_mag_pivot, on=['ID', 'Group', 'Mode', 'State'])\n",
    ")\n",
    "\n",
    "# Load the original dataset again (if needed)\n",
    "original_data_path = '/home/cerna3/neuroconn/data analyses/final_combined_data_compressed.xlsx'\n",
    "original_data = pd.read_excel(original_data_path)\n",
    "\n",
    "# Extract final_score_miniBEST, Practice, and Age from the original dataset\n",
    "original_variables = original_data[['ID', 'final_score_miniBEST', 'Practice', 'Age']]\n",
    "\n",
    "# Merge spatial and temporal dataframes\n",
    "combined_df = spatial_df.merge(temporal_df, on=['ID', 'Group', 'Mode', 'State'], how='outer')\n",
    "\n",
    "# Merge final_score_miniBEST, Practice, and Age into the new dataset\n",
    "combined_df_with_additional_info = pd.merge(combined_df, original_variables, on='ID', how='left')\n",
    "\n",
    "# Remove duplicates from temporal variables\n",
    "combined_df_with_additional_info = remove_temporal_duplicates(combined_df_with_additional_info)\n",
    "\n",
    "# --- Save and display ---\n",
    "output_file_path = '/home/cerna3/neuroconn/data analyses/combined_network_data.xlsx'\n",
    "combined_df_with_additional_info.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(combined_df_with_additional_info.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U test results have been saved to 'mann_whitney_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Univariate Pairwise Comparisons: Mann-Whitney U \n",
    "\n",
    "# Load the data from the provided Excel file\n",
    "file_path = '/home/cerna3/neuroconn/data analyses/combined_network_data.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Remove duplicates based on ID and Group\n",
    "data = data.drop_duplicates(subset=['ID', 'Group'])\n",
    "\n",
    "# Aggregate connectivity variables from the original dataset\n",
    "within_network_columns = [col for col in data.columns if '_w_mean' in col]\n",
    "within_network_tran_mag_columns = [col for col in data.columns if '_w_tran_mag' in col]\n",
    "between_network_columns = [col for col in data.columns if '_b_mean' in col]\n",
    "between_network_tran_mag_columns = [col for col in data.columns if '_b_tran_mag' in col]\n",
    "\n",
    "# Include temporal variables\n",
    "temporal_columns = ['Mean_Lifetime', 'Fractional_Occupancy', 'Mean_Interval_Length', 'Transition_Probabilities_PCA', 'Transition_Probabilities_Entropy']\n",
    "\n",
    "connectivity_columns = within_network_columns + within_network_tran_mag_columns + between_network_columns + between_network_tran_mag_columns + temporal_columns\n",
    "\n",
    "# Filter the data for YACs and OACs groups\n",
    "age_comparison_data = data[data['Group'].isin(['YACs', 'OACs'])]\n",
    "\n",
    "# Filter the data for OACs and TCOAs groups\n",
    "practice_comparison_data = data[data['Group'].isin(['OACs', 'TCOAs'])]\n",
    "\n",
    "def perform_mann_whitney_comparisons(data, group_column, connectivity_columns):\n",
    "    results = []\n",
    "    for column in connectivity_columns:\n",
    "        levels = data[group_column].unique()\n",
    "        comparisons = [(i, j) for i in levels for j in levels if i < j]\n",
    "        for i, j in comparisons:\n",
    "            group_i = data[data[group_column] == i][column].dropna()\n",
    "            group_j = data[data[group_column] == j][column].dropna()\n",
    "            statistic, p_value = stats.mannwhitneyu(group_i, group_j, alternative='two-sided')\n",
    "            mean_i, mean_j = group_i.mean(), group_j.mean()\n",
    "            median_i, median_j = group_i.median(), group_j.median()\n",
    "\n",
    "            # Calculate mean and median differences\n",
    "            mean_diff = mean_i - mean_j\n",
    "            median_diff = median_i - median_j\n",
    "\n",
    "            # Calculate Rank Biserial Correlation\n",
    "            n1, n2 = len(group_i), len(group_j)\n",
    "            rank_biserial_correlation = 2 * (statistic / (n1 * n2)) - 1\n",
    "\n",
    "            results.append([i, j, column, mean_i, mean_j, mean_diff, median_i, median_j, median_diff, statistic, p_value, rank_biserial_correlation])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['group1', 'group2', 'variable', 'mean1', 'mean2', 'mean_diff',\n",
    "                                                'median1', 'median2', 'median_diff', 'U-statistic', 'p-value',\n",
    "                                                'Rank Biserial Correlation (r)'])\n",
    "\n",
    "    # Apply multiple testing corrections\n",
    "    _, pvals_fdr, _, _ = multipletests(results_df['p-value'], method='fdr_bh')\n",
    "\n",
    "    # Add FDR-adjusted p-value column to results DataFrame\n",
    "    results_df.insert(results_df.columns.get_loc('p-value') + 1, 'FDR-adjusted p-value', pvals_fdr)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Perform Mann-Whitney U comparisons for age (YACs vs OACs)\n",
    "age_results = perform_mann_whitney_comparisons(age_comparison_data, 'Group', connectivity_columns)\n",
    "\n",
    "# Perform Mann-Whitney U comparisons for practice (OACs vs TCOAs)\n",
    "practice_results = perform_mann_whitney_comparisons(practice_comparison_data, 'Group', connectivity_columns)\n",
    "\n",
    "# Save the results to an Excel file with different tabs\n",
    "with pd.ExcelWriter('mann_whitney_results.xlsx') as writer:\n",
    "    age_results.to_excel(writer, sheet_name='age_comparison_results', index=False)\n",
    "    practice_results.to_excel(writer, sheet_name='practice_comparison_results', index=False)\n",
    "\n",
    "print(\"Mann-Whitney U test results have been saved to 'mann_whitney_results.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-hmm_env]",
   "language": "python",
   "name": "conda-env-anaconda3-hmm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
